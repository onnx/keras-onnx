{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert EfficientNet Model to ONNX and Inference with ONNX Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you'll be introduced to how to load a EfficientNet model (a Tensorflow Keras model), convert it to ONNX using Keras2onnx, and inference it for high performance using ONNX Runtime. In the following sections, we are going to use the pretrained EfficientNet model as an example. This EfficientNet model is used for image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites ##\n",
    "First we need a python environment before running this notebook.\n",
    "\n",
    "You can install [AnaConda](https://www.anaconda.com/distribution/) and [Git](https://git-scm.com/downloads) and open an AnaConda console when it is done. Then you can run the following commands to create a conda environment named cpu_env:\n",
    "\n",
    "```console\n",
    "conda create -n cpu_env python=3.7\n",
    "conda activate cpu_env\n",
    "```\n",
    "\n",
    "Finally, launch Jupyter Notebook and you can choose cpu_env as kernel to run this notebook.\n",
    "\n",
    "Let's install [Tensorflow](https://www.tensorflow.org/install), [OnnxRuntime](https://microsoft.github.io/onnxruntime/), Keras2Onnx and other packages like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install --quiet --upgrade tensorflow==2.1.0\n",
    "!{sys.executable} -m pip install --quiet --upgrade onnxruntime\n",
    "\n",
    "# Install keras2onnx from source to support tensorflow 2.1 models currently.\n",
    "!{sys.executable} -m pip install --quiet git+https://github.com/microsoft/onnxconverter-common\n",
    "!{sys.executable} -m pip install --quiet git+https://github.com/onnx/keras-onnx\n",
    "    \n",
    "# Install other packages used in this notebook.   \n",
    "!{sys.executable} -m pip install --quiet efficientnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Pretrained EfficientNet model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start to load the pretrained EfficientNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import efficientnet.tfkeras as efn\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from efficientnet.keras import center_crop_and_resize, preprocess_input\n",
    "from skimage.io import imread\n",
    "model = efn.EfficientNetB7(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tensorfow Inference\n",
    "\n",
    "Use one example to run inference using TensorFlow as baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "image = imread('panda.jpg')\n",
    "image_size = model.input_shape[1]\n",
    "x = center_crop_and_resize(image, image_size=image_size)\n",
    "x = preprocess_input(x)\n",
    "inputs = np.expand_dims(x, 0)\n",
    "expected = model.predict(inputs)\n",
    "result = decode_predictions(expected)\n",
    "print('classification_result_tfkeras = '+str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert the model to ONNX\n",
    "\n",
    "Now we use Keras2onnx to convert the model to ONNX format. It takes about 22 seconds for conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_model_path = 'keras_efficientNet.onnx'\n",
    "os.environ[\"TF_KERAS\"] = \"1\"\n",
    "\n",
    "import tensorflow\n",
    "import keras2onnx\n",
    "\n",
    "onnx_model = keras2onnx.convert_keras(model, model.name)\n",
    "keras2onnx.save_model(onnx_model, output_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference the ONNX Model with ONNX Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enable OpenMP environment variable for thread parallelism. Setting environment variables shall be done before importing onnxruntime. Otherwise, they might not take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "\n",
    "# You may change the settings in this cell according to Performance Test Tool result after running the whole notebook.\n",
    "use_openmp = True\n",
    "\n",
    "# ATTENTION: these environment variables must be set before importing onnxruntime.\n",
    "if use_openmp:\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(psutil.cpu_count(logical=True))\n",
    "else:\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "\n",
    "print('os_omp_num_threads='+os.environ[\"OMP_NUM_THREADS\"])\n",
    "\n",
    "os.environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference via onnxruntime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import onnxruntime\n",
    "import numpy\n",
    "  \n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "sess_options.intra_op_num_threads=psutil.cpu_count(logical=True)\n",
    "\n",
    "sess = onnxruntime.InferenceSession(output_model_path, sess_options)\n",
    "\n",
    "data = inputs.astype(np.float32)\n",
    "if isinstance(data, dict):\n",
    "    feed_input = data\n",
    "else:\n",
    "    data = data if isinstance(data, list) else [data]\n",
    "    input_names = sess.get_inputs()\n",
    "    feed = zip(sorted(i_.name for i_ in input_names), data)\n",
    "    feed_input = dict(feed)\n",
    "\n",
    "actual = sess.run(None, feed_input)\n",
    "result_onnx = decode_predictions(actual[0])\n",
    "print('classification_result_onnx = '+str(result_onnx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the tensorflow and onnx results, and verify the correctness here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"***** Verifying correctness (TensorFlow and ONNX Runtime) *****\")\n",
    "print('Results are close:', np.allclose(actual[0], expected, rtol=1e-05, atol=1e-04))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
